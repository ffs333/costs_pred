# Предксказание месячных трат пользователей по категориям.

## Описание данных
- data_train_transactions.csv - покупки клиентов за 2019 год
	- party_rk - уникальный id клиента
	- account_rk - уникальный id счёта
	- transaction_dttm - дата покупки
	- transaction_amt_rur - сумма покупки в рублях
	- category - категория траты
- data_test_transactions.csv - покупки клиентов за январь 2020 года. Для тестирования модели.
- data_party_x_socdem.csv - социально-демографические характеристики клиентов
- data_story_logs.csv - история взаимодействия клиентов с "историями" в приложении банка (лайки, дизлайки). 
	- party_rk - id клиента
	- date_time - дата взаимодействия
	- story_id - уникальный id истории
	- event - реакция клиента на историю (лайк, дизлайк)
- data_story_texts.csv - содержит тексты с историй
	- story_id - уникальный id истории (для матчинга с data_story_logs)
	- name - название истории
	- story_text - текст истории


Данные в исходном виде находятся в папке `data/raw_data`

## Метрика
Для оценки качества выбрана метрика попадания в +-N% от реальных затрат, по умолчанию использовал 20%. Так же, как альтернативная метика, и loss для обучения, использовалась MAE.
Достиг лучшего значения метрики 73% попаданий в +-20% от реальных расходов.

## Использование
Скрипт выполняет подготовку данных к обучению, обучение модели, валидацию, сохранение модели и конвертацию в ONNX.

Параметры скрипта находятся в `/configs/config.cfg`
В конфиге два модудя, **data** и **train** , data отвечает за подготовку данных и сохранение их, если нужно. Train за обучение модели.
### data
* **socdem, story_logs, story_texts, train_tr, test_tr** - пути к соответствующим csv файлам (str)
* **use_texts** - использовать ли текстовые данные (bool)
* **save_prepared** - сохранить ли данные после препроцессинга (bool). Сохранение в папку `/data/processed_data`
* **load_prepared** - использовать ли сохраненные данные (bool). Тогда препроцессинг выполняться не будет

### train
* **num_iterations** - количество итераций перебора для подбора параметров в optuna (int)
* **main_metric** - метрика, по которой будут оптимизироваться параметры. (**accuracy** or **mae**) (str)
* **convert_to_onnx** - переводить ли модель в ONNX (bool)
* **n_estimators** - количество деревьев в бустинге, в финальной модели будет n_estimators * 3 (int)
* **metric_thres** - порог для метрики accuracy (float)

### Запуск
* `pip install -r requirements.txt`
* `python3 run.py -c ./configs/config.cfg -v 1`
         
В скрипт передаётся путь к конфигу (обязательный параметр) и параметр verbose -v, 1 - будет выводить принты в ходе процесса, 0 - скроет. Не обязательный параметр, по умолчанию 1.

Модели сохранятся в папку `/models`

Если нужно использовать подготовленные данные, то предварительно запустить скрипт `python unzip_data.py`


### Предварительный анализ и визуализации в jupyter notebook'e vtb.ipynb

### Что можно улучшить?
* Лучше подобрать пороги для удаления выбросов
* Бороться с нуливыми значениями, помимо маскирования
* Сбор большего количества текстовых данных для моделей классификации на "продающие"/"не продающие" истории
